---
title: "Primera parte: Informe árboles de clasificación"
author: "David Rabanedo"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: lumen
    df_print: kable
    
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Explicación de la base de datos:
*Online Shoppers Intention*

The dataset consists of feature vectors belonging to 12,330 sessions.
The dataset was formed so that each session
would belong to a different user in a 1-year period to avoid
any tendency to a specific campaign, special day, user
profile, or period.

The dataset consists of 10 numerical and 8 categorical attributes.
The **Revenue** attribute can be used as the class label.


+ **Administrative**, **Administrative Duration**, **Informational**, **Informational Duration**, **Product Related** and **Product Related Duration** represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another.
+ The **Bounce Rate**, **Exit Rate** and **Page Value** features represent the metrics measured by "Google Analytics" for each page in the e-commerce site. 
+ The value of **Bounce Rate** feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session.
+ The value of **Exit Rate** feature for a specific web page is calculated as for all 
pageviews to the page, the percentage that were the last in the session. 
+ The **Page Value** feature represents the average value for a web page that a 
user visited before completing an e-commerce transaction.
+ The **Special Day** feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by 
considering the dynamics of e-commerce such as the duration between the order
date and delivery date. For example, for Valentina’s day, this value takes a 
nonzero value between February 2 and February 12, zero before and after this 
date unless it is close to another special day, and its maximum value of 1 on 
February 8.
+ The dataset also includes operating system, browser, region, traffic
type, visitor type as returning or new visitor, a Boolean value indicating 
whether the date of the visit is weekend, and month of the year

Para más información, ver el pdf Sakar2019, en esta misma carpeta.

## Un primer acercamiento a los datos

```{r}
##Primero asignamos la variable datos a nuestra base de datos, para poder asi estudiarla
library(rpart)
library(rpart.plot)
library(ggplot2)
datos <- read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv', header = TRUE, sep =',')

##Vamos a crear una nueva columna, asignando a los datos de la variable Revenue
##el valor Si o No (si se produce la compra o no), si dicha variable vale TRUE ó 
##FALSE, respectivamente. Además lo creamos de forma que los datos sean de tipo factor,
##ya quevamos a elaborar un árbol de clasificación a partir del paquete rpart.
compra <- factor(datos$Revenue , levels = c(FALSE,TRUE), labels = c("NO", "SI"))
datos$y <- compra

##Reasignamos los valores que son cualitativos a datos tipo factor
datos$OperatingSystems = factor(datos$OperatingSystems)
datos$Browser = factor(datos$Browser)
datos$Region = factor(datos$Region)
datos$TrafficType = factor(datos$TrafficType)
datos$VisitorType = factor(datos$VisitorType)
datos$Weekend = factor(datos$Weekend) 
datos$Month = factor(datos$Month, labels = c('Aug', 'Dec', 'Feb', 'Jul', 'June', 'Mar', 'May', 'Nov', 'Oct', 'Sep'))
datos$Revenue = factor(datos$Revenue)

##Veamos como terminaron los 100 primeros casos
datos$y[1:100]
```

## Árbol por defecto y su eficiencia sobre el conjunto
Ahora que ya tenemos nuestros datos, vamos a elaborar los arboles y realizar un estudio para decidir cual de ellos es el mejor. Veamos un primer ejemplo, realizado con los parametros default del paquete rpart, que son:

- **xval = 10** (conjuntos para hacer validación cruzada)  
- **cp = 0.01** (parámetro de complejidad)  
- **split formula = GINI** (fórmula para realizar los splits, puede ser *GINI* o *information*)  
- **minsplit = 20** (mínimo de observaciones en un nodo para que se divida)
- **minbucket = round(minsplit/3)** (mínimo de observaciones en un nodo terminal)  
- **maxcompete = 4** (muestra las siguientes 4 variables que daban el mejor split)  
- **maxsurrogate = 5** (el máximo número de variables surrogadas que mantiene en cada nodo. Recordar que las variables surrogadas son los siguientes mejores splits, y que estos se usan en caso de que haya información faltante)  
- **usesurrogate = 2** (tipo de trato que se le da a la información faltante, en este caso (ver libro))  
- **surrogatestyle = 0**  
- **maxdepth = 30**;  

### Construcción del árbol

```{r}
set.seed(8594)
tcar_default <- rpart( y ~ Administrative + Administrative_Duration + Informational
               + Informational_Duration +  ProductRelated + ProductRelated_Duration
               + BounceRates + ExitRates + PageValues + SpecialDay + Month 
               + OperatingSystems + Browser + Region +  TrafficType + VisitorType
               + Weekend  , data=datos, method = 'class')
rpart.plot(tcar_default, main = "Default", box.palette = c("red","green"))

```

### Observaciones bien clasificadas (información privilegiada)

Veamos cuantas observaciones clasifica bien este arbol , el default, sobre el conjunto de datos con el que se ha construido (tiene información privilegiada):

```{r}
tcar_default_table = table(predict(tcar_default, t="class"), datos$y)
tcar_default_table
aciertos_tcar_default = tcar_default_table[1,1]+tcar_default_table[2,2]
fallos_tcar_default = 12330 - aciertos_tcar_default
aciertos_tcar_default
fallos_tcar_default
```

Tenemos pues que este árbol clasifica de forma correcta el siguiente porcentaje de observaciones:
```{r echo=FALSE}
paste0("El acierto, con información privilegiada, es del ", (aciertos_tcar_default/12330)*100, "%")
```

### Observaciones bien clasificadas (Cross-Validation)

Vamos a utilizar el código que veremos más adelante, en el apartado de Poda y detección del árbol óptimo, para calcular el error de validación cruzada. Se explicará más adelante con mucho más detalle:

```{r}
#############Aprovechamos el bucle que ya habíamos creado#######################
## Primero elegimos el número de capas de cross-validation
fold = 10
## Observaciones
n = 12330
## Tamanño de la muestra test
n_test = n/fold
## Tamaño de la muestra trainning
n_tr = n - n_test 
vn = 1:n
## Hacemos una tabla para almacenar los datos
v_tabla_optimo = matrix(rep(NA,fold*4), nrow = fold)
## Cogemos el parametro por defecto
cp_optimo = 0.01
min_nod <- 1
min_cp <- 0
cv <- 10

for (i in 1:fold ) {
  set.seed(8594)
  i_test = 1:n_test + (i - 1)*n_test
  i_tr = vn[-i_test]
  datos_tr = datos[i_tr,]
  datos_test = datos[i_test,]
  tcar_eficacia <- rpart( y ~ Administrative + Administrative_Duration + Informational
                     + Informational_Duration +  ProductRelated + ProductRelated_Duration
                     + BounceRates + ExitRates + PageValues + SpecialDay + Month 
                     + OperatingSystems + Browser + Region +  TrafficType + VisitorType
                     + Weekend, data=datos_tr, method = 'class',
                     control = rpart.control(xval = cv, minbucket = min_nod, cp = min_cp))
  tcar_pruned_eficacia = prune(tcar_eficacia, cp_optimo)
  tcar_pruned_table = table(predict(tcar_pruned_eficacia, datos_test, t="class",), datos_test$y)
  v_tabla_optimo[i,] = matrix(tcar_pruned_table, nrow = 1)
  
  
}
v_suma_optimo = apply(v_tabla_optimo,2,sum)
aciertos_cv_optimo = sum(v_suma_optimo[c(1,4)])
fallos_cv_optimo = sum(v_suma_optimo[c(2,3)])
p_aciertos_cv_optimo = (aciertos_cv_optimo/12330)*100

paste0("El acierto en las muestras por cross-validation es del ", round(p_aciertos_cv_optimo,6), "%, con el parámetro de complejidad de ", round(cp_optimo, 6))
```


## Contrucción del árbol total y eficiencia sobre el conjunto

Vamos entonces a hacer el árbol más largo posible, garantizándonos así de que podemos elegir el mejor subárbol. 

### Construcción del árbol

```{r}
min_nod <- 1
min_cp <- 0
cv <- 10
set.seed(8594)

tcar_total <- rpart( y ~ Administrative + Administrative_Duration + Informational
                     + Informational_Duration +  ProductRelated + ProductRelated_Duration
                     + BounceRates + ExitRates + PageValues + SpecialDay + Month 
                     + OperatingSystems + Browser + Region +  TrafficType + VisitorType
                     + Weekend, data=datos, method = 'class',
                     control = rpart.control(xval = cv, minbucket = min_nod, cp = min_cp))
```

Vamos ahora a ver como se han repartido las observaciones y como es la estructura de nuestro árbol, de forma que podamos elegir los mejores parámetros para realizar el podado. Para un resumen exhaustivo, podemos ejecutar el siguiente comando:

```{r eval=FALSE}
summary(tcar_total)
```


### Observaciones bien clasificadas (información privilegiada)

Veamos cuantas observaciones clasifica bien este árbol, el arbol completo:
```{r}
tcar_total_table = table(predict(tcar_total, t="class"), datos$y)
aciertos_tcar_total = tcar_total_table[1,1]+tcar_total_table[2,2]
fallos_tcar_total = 12330 - aciertos_tcar_total 
```
Tenemos pues que este árbol clasifica de forma correcta el siguiente porcentaje de observaciones:
```{r echo=FALSE}

paste0("El acierto, con información privilegiada, es del ",(aciertos_tcar_total/12330)*100, "%")
```

Vemos como este árbol, a diferencia del default, se sobreajusta mucho a los datos, lo cual nos puede llegar a generar un problema a a la hora de calcular nuevas clasificaciones a nuevos conjuntos de datos. Para evitar esto vamos a calcular mediante dos métodos diferentes cual es el parámetro óptimo para realizar la poda de nuestro árbol, de forma que nos ayude a hacer una buena clasificación de nuevos datos entrantes, pero antes de ello, veamos como se comporta este full-grown-tree ante la Validación Cruzada.

### Observaciones bien clasificadas (Cross-Validation)

```{r}
#############Aprovechamos el bucle que ya habíamos creado#######################
## Primero elegimos el número de capas de cross-validation
fold = 10
## Observaciones
n = 12330
## Tamanño de la muestra test
n_test = n/fold
## Tamaño de la muestra trainning
n_tr = n - n_test 
vn = 1:n
## Hacemos una tabla para almacenar los datos
v_tabla_optimo = matrix(rep(NA,fold*4), nrow = fold)
## Cogemos los parametros para hacer un fullgrowntree
min_nod <- 1
min_cp <- 0
cv <- 10

for (i in 1:fold ) {
  set.seed(8594)
  i_test = 1:n_test + (i - 1)*n_test
  i_tr = vn[-i_test]
  datos_tr = datos[i_tr,]
  datos_test = datos[i_test,]
  tcar_efectivo <- rpart( y ~ Administrative + Administrative_Duration + Informational
                     + Informational_Duration +  ProductRelated + ProductRelated_Duration
                     + BounceRates + ExitRates + PageValues + SpecialDay + Month 
                     + OperatingSystems + Browser + Region +  TrafficType + VisitorType
                     + Weekend, data=datos_tr, method = 'class',
                     control = rpart.control(xval = cv, minbucket = min_nod, cp = min_cp))
  
  tcar_pruned_table = table(predict(tcar_efectivo, datos_test, t="class",), datos_test$y)
  v_tabla_optimo[i,] = matrix(tcar_pruned_table, nrow = 1)
  
  
}
v_suma_optimo = apply(v_tabla_optimo,2,sum)
aciertos_cv_optimo = sum(v_suma_optimo[c(1,4)])
fallos_cv_optimo = sum(v_suma_optimo[c(2,3)])
p_aciertos_cv_optimo = (aciertos_cv_optimo/12330)*100

paste0("El acierto en las muestras por cross-validation es del ", round(p_aciertos_cv_optimo,6), "%, con el árbol totalmente crecido")
```

Como observamos, al haberse sobreajustado a los datos, ahora genera que la utilización de ese tipo de árbol para la clasificación nos de más errores que en el default.

## Elección del mejor pámetro de complejidad. Dos caminos.

### Minimizar el *xerror*

Aquí tenemos toda la información, pero lo que a nosotros nos interesa es elegir el mejor parámetro de complejidad (*cp*) para realizar la poda a nuestro árbol. Vamos a ver cuales son los Óptimos mostrándolos en una tabla.
```{r}
printcp(tcar_total)
```
 
 Vamos pues, en primer lugar, a buscar el árbol que tenga el menor error de validación cruzada, es decir, aquel que minimiza el parámetro *xerror* de la tabla.
 
```{r}
cp_optimo = sqrt(tcar_total$cptable[which.min(tcar_total$cptable[,"xerror"]),"CP"]*tcar_total$cptable[which.min(tcar_total$cptable[,"xerror"])-1,"CP"])
cp_optimo
```

##### *Poda y obtención del árbol óptimo (x-error)*

Vamos a podar el arbol en busca de que este árbol sea óptimo, utilizando en primer lugar el parámetro de complejidad que minimiza el xerror, que despúes compararemos con los demás ya que no tiene por qué ser el óptimo, ya que normalmente este corresponde al elegido mediante el método 1SE. Comencemos:

```{r}
#Vamos a utilizar los valores de la tabla para elegir dicho cp, ya que así somos
#completamente exactos

tcar_pruned_minxerror = prune(tcar_total, cp = sqrt(tcar_total$cptable[which.min(tcar_total$cptable[,"xerror"]),"CP"]*tcar_total$cptable[which.min(tcar_total$cptable[,"xerror"])-1,"CP"]))
rpart.plot(tcar_pruned_minxerror, main = "Best pruned Tree (x-error)", box.palette = c("red","green"))
```

#### *Observaciones bien clasificadas mínimo x-error (información privilegiada)*

Veamos cuantas observaciones clasifica bien este árbol, que es el que tiene mejor parámetro de complejidad según el método de validación cruzada:
```{r}
tcar_pruned_minxerror_table = table(predict(tcar_pruned_minxerror, t="class"), datos$y)
aciertos_tcar_pruned_minxerror = tcar_pruned_minxerror_table[1,1]+tcar_pruned_minxerror_table[2,2]
fallos_tcar_pruned_minxerror = 12330 - aciertos_tcar_pruned_minxerror
```

Tenemos pues que este árbol clasifica de forma correcta el siguiente porcentaje de observaciones:
```{r echo=FALSE}

paste0("El acierto, con información privilegiada, es del ", (aciertos_tcar_pruned_minxerror/12330)*100, "%")
```

#### *Observaciones bien clasificadas mínimo x-error (Cross-Validation)*

Vamos a hacer validación cruzada con el mejor árbol, según el xerror, que hemos podado con el fin de comparar el acierto:

```{r}
## Primero elegimos el número de capas de cross-validation
fold = 10
## Observaciones
n = 12330
## Tamanño de la muestra test
n_test = n/fold
## Tamaño de la muestra trainning
n_tr = n - n_test 
vn = 1:n
## Hacemos una tabla para almacenar los datos
v_tabla_optimo = matrix(rep(NA,fold*4), nrow = fold)
## Volvemos a extraer el cp óptimo ya que lo utilizaremos para construir el árbol
cp_optimo = sqrt(tcar_total$cptable[which.min(tcar_total$cptable[,"xerror"]),"CP"]*tcar_total$cptable[which.min(tcar_total$cptable[,"xerror"])-1,"CP"])
cp_optimo
```

Ya tenemos todos los datos, vamos ahora a realizar el método de cross-validation para ver que número de observaciones clasifica bien y ver así la eficiencia del árbol que hemos generado.

```{r}
for (i in 1:fold ) {
  set.seed(8594)
  i_test = 1:n_test + (i - 1)*n_test
  i_tr = vn[-i_test]
  datos_tr = datos[i_tr,]
  datos_test = datos[i_test,]
  tcar_eficacia <- rpart( y ~ Administrative + Administrative_Duration + Informational
                         + Informational_Duration +  ProductRelated + ProductRelated_Duration
                         + BounceRates + ExitRates + PageValues + SpecialDay + Month 
                         + OperatingSystems + Browser + Region +  TrafficType + VisitorType
                         + Weekend, data=datos_tr, method = 'class',
                         control = rpart.control(xval = cv, minbucket = min_nod, cp = min_cp))
  tcar_pruned_eficacia = prune(tcar_eficacia, cp_optimo)
  tcar_pruned_table = table(predict(tcar_pruned_eficacia, datos_test, t="class",), datos_test$y)
  v_tabla_optimo[i,] = matrix(tcar_pruned_table, nrow = 1)
  
  
}
``` 

Extraigamos ahora los datos, utilizando la tabla que habíamos generado.

```{r}
v_tabla_optimo
v_suma_optimo = apply(v_tabla_optimo,2,sum)
aciertos_cv_optimo = sum(v_suma_optimo[c(1,4)])
fallos_cv_optimo = sum(v_suma_optimo[c(2,3)])
p_aciertos_cv_optimo = (aciertos_cv_optimo/12330)*100
```

Veamos pues cuantas observaciones ha clasificado bien nuestro árbol que minimiza el xerror.

```{r echo=FALSE}
paste0("El acierto en las muestras por cross-validation es del ", round(p_aciertos_cv_optimo,6), "%, con el parámetro de complejidad de ", round(cp_optimo, 6))
```


### Minimizar ultilizando el método *1-SE*

Podemos mirarlo directamente, viendo primero:

$$ xerror_k = \min{xerror}$$
Donde $$xstd_k$$ es el error estándar asociado a ese xerror minímo, en la posición k de la tabla.
Queremos entonces encontrar el primer cp que cumple:
$$  xerror_{cp} < xerror_k + xstd_k $$
De esta forma encontramos el cp óptimo por el método 1SE.

```{r}
one_se = tcar_total$cptable[which.min(tcar_total$cptable [,4]),4] + tcar_total$cptable[which.min(tcar_total$cptable [,4]),5]
a = tcar_total$cptable[,4] > one_se
cp_optimo_pos = min(which(a == FALSE))
cp_se = sqrt(tcar_total$cptable[cp_optimo_pos,"CP"]*tcar_total$cptable[cp_optimo_pos-1,"CP"])
one_se
```

También podemos aplicar el siguiente comando, que nos muestra la mejor decisión utilizando el método 1SE. Esta decisión se corresponde al primer valor (el que esté mas a la izquierda debajo de la linea horizontal)
```{r}
plotcp(tcar_total)
```


##### *Poda y obtención del árbol óptimo (1SE)*

Veamos ahora como se comporta nuestro árbol utilizando el parámetro elegido por la regla 1SE:

```{r}
#Vamos a utilizar los valores de la tabla para elegir dicho cp, ya que así somos
#completamente exactos

tcar_pruned_se = prune(tcar_total, cp = cp_se)
rpart.plot(tcar_pruned_se, main = "Best pruned Tree (1-SE)", box.palette = c("red","green"))
```

#### *Observaciones bien clasificadas 1-SE (información privilegiada)*

Veamos cuantas observaciones clasifica bien este árbol, que es el que tiene mejor parámetro de complejidad según el método 1SE:
```{r}
tcar_pruned_se_table = table(predict(tcar_pruned_se, t="class"), datos$y)
aciertos_tcar_pruned_se = tcar_pruned_se_table[1,1]+tcar_pruned_se_table[2,2]
fallos_tcar_pruned_se = 12330 - aciertos_tcar_pruned_se
```

Tenemos pues que este árbol clasifica de forma correcta el siguiente porcentaje de observaciones:
```{r echo=FALSE}

paste0("El acierto, con información privilegiada, es del ", (aciertos_tcar_pruned_se/12330)*100, "%")
```

#### *Observaciones bien clasificadas 1-SE (Cross-Validation)*

Vamos a hacer validación cruzada con el mejor árbol, según el 1SE, que hemos podado con el fin de comparar el acierto:

```{r}
## Primero elegimos el número de capas de cross-validation
fold = 10
## Observaciones
n = 12330
## Tamanño de la muestra test
n_test = n/fold
## Tamaño de la muestra trainning
n_tr = n - n_test 
vn = 1:n
## Hacemos una tabla para almacenar los datos
v_tabla_se = matrix(rep(NA,fold*4), nrow = fold)
```

Ya tenemos todos los datos, vamos ahora a realizar el método de cross-validation para ver que número de observaciones clasifica bien y ver así la eficiencia del árbol que hemos generado.

```{r}
for (i in 1:fold ) {
  set.seed(8594)
  i_test = 1:n_test + (i - 1)*n_test
  i_tr = vn[-i_test]
  datos_tr = datos[i_tr,]
  datos_test = datos[i_test,]
  tcar_eficacia <- rpart( y ~ Administrative + Administrative_Duration + Informational
                         + Informational_Duration +  ProductRelated + ProductRelated_Duration
                         + BounceRates + ExitRates + PageValues + SpecialDay + Month 
                         + OperatingSystems + Browser + Region +  TrafficType + VisitorType
                         + Weekend, data=datos_tr, method = 'class',
                         control = rpart.control(xval = cv, minbucket = min_nod, cp = min_cp))
  tcar_pruned_eficacia = prune(tcar_eficacia, cp_se)
  tcar_pruned_table = table(predict(tcar_pruned_eficacia, datos_test, t="class",), datos_test$y)
  v_tabla_se[i,] = matrix(tcar_pruned_table, nrow = 1)
  
  
}
``` 

Extraigamos ahora los datos, utilizando la tabla que habíamos generado.

```{r}
v_suma_se = apply(v_tabla_se,2,sum)
aciertos_cv_se = sum(v_suma_se[c(1,4)])
fallos_cv_se = sum(v_suma_se[c(2,3)])
p_aciertos_cv_se = (aciertos_cv_se/12330)*100
```

Veamos pues cuantas observaciones ha clasificado bien nuestro árbol que minimiza la regla 1SE.

```{r echo=FALSE}
paste0("El acierto en las muestras por cross-validation es del ", round(p_aciertos_cv_se,6), "%, con el parámetro de complejidad de ", round(cp_se, 6))
```

### Comparación de los dos árboles con los demás

Veamos la eficiencia mediante el método de crossvalidation de cada uno de los parámetros que aparecen en la cptable del tcar_total. Veamos así si nuestra elección era la que más eficiencia tiene según este método:

```{r}
##Vamos a hacer la comparativa con todos los valores de la cptable, donde se supone que están 
##los parámetros óptimos

cp_todos_v = c(Inf,tcar_total$cptable[,1])
par_comp = rep(NA,length(cp_todos_v)-1)
eficiencia = rep(NA,length(cp_todos_v)-1)
nodos = rep(NA,length(cp_todos_v)-1)
tabla_comparacion = data.frame(Parametro_complejidad = par_comp, Eficiencia = eficiencia, Nodos = nodos)
nodos_todos_v = tcar_total$cptable[,2]

for (j in 1:(length(cp_todos_v)-1)){
  fold = 10
  n = 12330
  n_test = n/fold
  n_tr = n - n_test 
  vn = 1:n
  min_nod <- 1
  min_cp <- 0
  cv <- 10
  v_tabla = matrix(rep(NA,fold*4), nrow = fold)
  cp_todos = sqrt(cp_todos_v[j]*cp_todos_v[j+1])
  
  for (i in 1:fold ) {
    set.seed(8594)
    i_test = 1:n_test + (i - 1)*n_test
    i_tr = vn[-i_test]
    datos_tr = datos[i_tr,]
    datos_test = datos[i_test,]
    tcar_todos <- rpart( y ~ Administrative + Administrative_Duration + Informational
                         + Informational_Duration +  ProductRelated + ProductRelated_Duration
                         + BounceRates + ExitRates + PageValues + SpecialDay + Month 
                         + OperatingSystems + Browser + Region +  TrafficType + VisitorType
                         + Weekend, data=datos_tr, method = 'class',
                         control = rpart.control(xval = cv, minbucket = min_nod, cp = min_cp))
    tcar_pruned = prune(tcar_todos, cp_todos)
    tcar_pruned_table = table(predict(tcar_pruned, datos_test, t="class",), datos_test$y)
    v_tabla[i,] = matrix(tcar_pruned_table, nrow = 1)
  }
  v_suma = apply(v_tabla,2,sum)
  aciertos_cv = sum(v_suma[c(1,4)])
  fallos_cv = sum(v_suma[c(2,3)])
  p_aciertos_cv = (aciertos_cv/12330)*100
  
  tabla_comparacion$Parametro_complejidad[j] = cp_todos
  tabla_comparacion$Eficiencia[j] = p_aciertos_cv
  tabla_comparacion$Nodos[j] = nodos_todos_v[j]
  
  
  
}
ggplot(tabla_comparacion) +
  geom_line(aes(x = Nodos, y = Eficiencia, col = "Evolución general")) +
  geom_line(aes(x = Nodos, y = p_aciertos_cv_optimo, col= "Método x-error")) +
  geom_line(aes(x = Nodos, y = p_aciertos_cv_se, col = "Método 1SE")) + scale_color_discrete("Leyenda") +
  labs(title='Probabilidad de acierto en función del número de nodos',
       x='Número de nodos',
       y='Probabilidad de acierto') 
  

```

Con esto nos damos cuenta que sobreajustar el arbol no ayuda para nada a realizar una mejor predicción.
Veamos cuales son los valores máximos de esta tabla que hemos creado:

Máximo general:
```{r}
tabla_comparacion[which.max(tabla_comparacion$Eficiencia),]
``` 

Valor óptimo obtenido por la regla 1SE:
```{r}
p_aciertos_cv_se
``` 

Valor óptimo obtenido minimizando el x-error:
```{r}
p_aciertos_cv_optimo
``` 
## Resolución mediante bagging

Ahora que ya hemos realizado todos los cálculos y comparado la eficiencia de los
árboles en función de los parámetros de complejidad, vamos a implementar el 
bagging. Esta técnica es un método ENSEMBLE que nos permite mejorar la eficiencia
de nuestro método mediante la replicación del conjunto. Para más información, 
leer el libro de Izenmann. Para ello vamos a utilizar la siguiente librería.

```{r , warning=FALSE, message=FALSE}
library(adabag)
```

Utilizando el paquete Adabag, vamos a analizar los datos ya vistos. Con este comando, The data are divided into v non-overlapping subsets of roughly equal size. Then, bagging is applied on (v-1) of the subsets. Finally, predictions are made for the left out subsets, and the process is repeated for each of the v subsets.



```{r}
set.seed(8594)
cv_bagging = 10
n_trees_bagging = 100
tcar_bagging = bagging.cv(y ~ Administrative + Administrative_Duration + Informational
        + Informational_Duration +  ProductRelated + ProductRelated_Duration
        + BounceRates + ExitRates + PageValues + SpecialDay + Month 
        + OperatingSystems + Browser + Region +  TrafficType + VisitorType
        + Weekend, datos, v = cv_bagging, mfinal = n_trees_bagging)
```
 
 Veamos cuanto es el porcentaje de acierto según este método:

```{r, echo = FALSE}
Porcentaje_aciertos_bagging_cv = (1-tcar_bagging$error)*100
paste0("El acierto en las muestras haciendo el método de random forest es del ", Porcentaje_aciertos_bagging_cv, "%")
```
 
## Resolución mediante Random Forest
 
Vamos a dar un paso más y implementar las técnicas de random forest para realizar la clasificación del conjunto elegido y ver como de bien clasifica nuestro conjunto. Para ello cargamos la librería ranger:
 
```{r , warning=FALSE, message=FALSE}
library(ranger)
```

Vamos ahora a realizar el algoritmo correspondiente a los random forest:

```{r , warning=FALSE, message=FALSE}
n_trees_rf = 1000 #Numero de árboles a utilizar
min_nod_size_rf = 1 #Queremos un fullgrown tree, para reducir el sesgo
r_forest = ranger(
    formula = Revenue ~ Administrative + Administrative_Duration + Informational
                  + Informational_Duration +  ProductRelated + ProductRelated_Duration
                  + BounceRates + ExitRates + PageValues + SpecialDay + Month 
                  + OperatingSystems + Browser + Region +  TrafficType + VisitorType
                  + Weekend,
    data = datos, #Conjunto de datos
    num.trees = n_trees_rf,
    mtry = NULL, #Number of variables to possibly split at in each node, Default
    #is the (rounded down) square root of the number variables. 
    importance = "impurity_corrected",
    write.forest = TRUE, #Si guarda el árbol para después predecir con ese árbol
    probability = FALSE,
    min.node.size = min_nod_size_rf, #Queremos un fullgrown tree, para reducir el sesgo
    max.depth = NULL, #NULL nos da crecimiento ilimitado
    replace = TRUE, #La muestra la hace con remplazamiento
    case.weights = NULL, #Para oversamplear ciertos valores
    class.weights = NULL, #Pesos para el outcome
    splitrule = "gini",
    split.select.weights = NULL,
    always.split.variables = NULL, #Para meter esas variables siempre en el
    #split a mayores de las que están en mtry
    regularization.factor = 1,
    keep.inbag = FALSE,
    inbag = NULL, #Manually set observations per tree. List of size num.trees, 
    #containing inbag counts for each observation. Can be used for stratified sampling.
    oob.error = TRUE,
    num.threads = NULL,
    save.memory = FALSE, #Memory saving split mode
    verbose = TRUE, #	Show computation status and estimated runtime
    seed = 8594,
    dependent.variable.name = NULL, #Name of dependent variable, needed if no formula given
    classification = TRUE)
```

### Comparación entre diferentes bosques

 Habiendo hecho esto, sólo nos queda saber cómo de bien funciona. Podríamos haber hecho una training sample y una test sample, pero dado que el propio algoritmo nos proporciona el error cometido sobre las Out-Of-The-Bag Observations, vamos a aprovecharlo y ver como clasifica nuestro random forest nuestro propio conjunto (No hay información privilegiada, luego es legítimo)


```{r, echo=FALSE}
Porcentaje_aciertos_rf_cv = (1-r_forest$prediction.error)*100
paste0("El acierto en las muestras haciendo random forest es del ", Porcentaje_aciertos_rf_cv, "%")
```

Veamos ahora, ademas de con los parámetros default, que pasaría si modificamos el número de variables elegidas por nodo, como nos decía el libro. Tenemos que en este caso sqrt(17) es aproximadamente 4 (redondeando). Veamos que pasa tomando los otros dos valores, tanto $2*\sqrt17$  que es 8 (redondeando), como $0.5*\sqrt17$.

Aprovechando, también vamos a variar el número de árboles utilizados.

```{r}

n_trees_rf_bucle = c(100,250,500,750,1000,2000,3000) #Numero de árboles a utilizar
mtry_rf_bucle = c(round(0.5*sqrt(ncol(datos)-2)), round(sqrt(ncol(datos)-2)), round(2*sqrt(ncol(datos)-2)), 6)
min_nod_size_rf = 1 #Queremos un fullgrown tree, para reducir el sesgo
tabla_comparacion = data.frame(Splits_por_nodo = rep(NA, length(n_trees_rf_bucle)*length(mtry_rf_bucle)), Numero_arboles = rep(NA,length(n_trees_rf_bucle)*length(mtry_rf_bucle)),
                               Porcentaje_de_aciertos = rep(NA,length(n_trees_rf_bucle)*length(mtry_rf_bucle)))
rf_tabla = matrix(rep(NA,length(n_trees_rf_bucle)*length(mtry_rf_bucle)), nrow = length(n_trees_rf_bucle))
k = 0
for (i in 1:length(n_trees_rf_bucle)) {
  for (j in 1:length(mtry_rf_bucle)) {
    n_trees_int = n_trees_rf_bucle[i]
    mtry_int = mtry_rf_bucle[j]
    r_forest_buc = ranger(
      formula = Revenue ~ Administrative + Administrative_Duration + Informational
      + Informational_Duration +  ProductRelated + ProductRelated_Duration
      + BounceRates + ExitRates + PageValues + SpecialDay + Month 
      + OperatingSystems + Browser + Region +  TrafficType + VisitorType
      + Weekend,
      data           = datos, 
      num.trees      = n_trees_int,
      importance     = "impurity_corrected",
      mtry           = mtry_int ,
      seed           = 8594,
      classification = TRUE)
    rf_tabla[i,j] = round((1-r_forest_buc$prediction.error)*100, digits=3)
    k = k + 1
    tabla_comparacion$Porcentaje_de_aciertos[k] = round((1-r_forest_buc$prediction.error)*100, digits= 3)
    tabla_comparacion$Splits_por_nodo[k] = mtry_rf_bucle[j]
    tabla_comparacion$Numero_arboles[k]  = n_trees_rf_bucle[i]
    
  }
}    

tabla_comparacion$Porcentaje_de_aciertos = as.factor(tabla_comparacion$Porcentaje_de_aciertos)

```

En esta tabla (matriz) mostramos los datos de cada uno de los diferentes elecciones de splits (columnas) asi como del numero de árboles utilizados (las diferentes filas)

```{r, eval=FALSE}
rf_tabla
```
 Tenemos que el valor que minimiza el error, es aquel que maximiza los aciertos, en este caso:
 
```{r ,echo = FALSE}
Valor_optimo = rf_tabla[which.max(rf_tabla)]
paste0("El acierto en las muestras haciendo random forest con los parámetros variados es del ", Valor_optimo , "%")

```
Veamos los datos representados en el dataframe para mejor interpretación:

```{r, eval=FALSE}
tabla_comparacion
```
 
### Interpretación gráfica 
 
Para una mejor interpretación vamos a hacer una gráfica con los diferentes datos:
 
```{r}
v2_rf = rf_tabla[,1]
v4_rf = rf_tabla[,2]
v8_rf = rf_tabla[,3]
v6_rf = rf_tabla[,4]
datos_grafica =  data.frame( "Sub" = v2_rf,
                             "Acc" = v4_rf,
                             "Sup" = v8_rf,
                             "Una" = v6_rf,
                             "Numero_de_arboles" = n_trees_rf_bucle)
ggplot(datos_grafica) +
  geom_line(aes(x = Numero_de_arboles, y = Sub, col = "2 var por split")) + 
  geom_line(aes(x = Numero_de_arboles, y = Acc, col = "4 var por split")) +
  geom_line(aes(x = Numero_de_arboles, y = Una, col = "6 var por split")) +
  geom_line(aes(x = Numero_de_arboles, y = Sup, col = "8 var por split")) + 
  scale_color_discrete("Número de variables por Split") +
  labs(title='Probabilidad de acierto en función del número de árboles, según el split',
       x='Número de árboles',
       y='Probabilidad de acierto') 

```


### Variables significativas: modelo alternativo

 Finalmente, veamos los p-values y la "importance" de las variables, del bosque que mejor se comporta sobre nuestros datos. En primer lugar encontremos el bosque que mejor se comporta, en nuestro caso:

```{r}
tabla_comparacion[which.max(tabla_comparacion$Porcentaje_de_aciertos),]
```

Hagamos ahora ese bosque:

```{r}
n_trees_op = tabla_comparacion[which.max(tabla_comparacion$Porcentaje_de_aciertos),]$Numero_arboles
mtry_op = tabla_comparacion[which.max(tabla_comparacion$Porcentaje_de_aciertos),]$Splits_por_nodo
r_forest_op = ranger(
  formula = Revenue ~ Administrative + Administrative_Duration + Informational
  + Informational_Duration +  ProductRelated + ProductRelated_Duration
  + BounceRates + ExitRates + PageValues + SpecialDay + Month 
  + OperatingSystems + Browser + Region +  TrafficType + VisitorType
  + Weekend,
  data           = datos, 
  num.trees      = n_trees_op,
  importance     = "impurity_corrected",
  mtry           = mtry_op,
  seed           = 8594,
  classification = TRUE)
```


```{r}
importance_pvalues(r_forest_op,method = "janitza")
```

Viendo cuales son las variables de importancia, creamos dos árboles, uno con las variables significativas y otro con las que no lo son, de cara a ver como se comportan.

#### **Variables no significativas:**

Creamos el árbol y después evaluamos cómo de bien se comporta clasificando los datos:
```{r}
r_forest_noimp = ranger(
  formula = Revenue ~ SpecialDay + OperatingSystems + Browser + Region +  TrafficType
  + Weekend,
  data           = datos, 
  num.trees      = 1000,
  importance     = "impurity_corrected",
  mtry           = 4,
  seed           = 8594,
  classification = TRUE)
```

Veamos ahora el rendimiento de este nuevo árbol que hemos creado:
```{r}
Porcentaje_aciertos_rf_cv = (1-r_forest_noimp$prediction.error)*100
paste0("El acierto en las muestras haciendo random forest es del ", Porcentaje_aciertos_rf_cv, "%")
``` 

#### **Variables significativas:**

Veamos ahora como se comporta el árbol que hemos creado a partir de las variables significativas, veremos también como se comporta comparándolo con el general:

```{r}
r_forest_imp = ranger(
  formula = Revenue ~ Administrative + Administrative_Duration + Informational
  + Informational_Duration +  ProductRelated + ProductRelated_Duration
  + BounceRates + ExitRates + PageValues +  Month + VisitorType,
  data           = datos, 
  num.trees      = 1000,
  importance     = "impurity_corrected",
  mtry           = 4,
  seed           = 8594,
  classification = TRUE)
``` 

Veamos ahora como es el rendimiento de nuestro nuevo árbolito:
```{r}
Porcentaje_aciertos_rf_cv = (1-r_forest_imp$prediction.error)*100
paste0("El acierto en las muestras haciendo random forest es del ", Porcentaje_aciertos_rf_cv, "%")
``` 

### Comparación final: ¿Cúal es el mejor árbol?¿Cúal debemos utilizar?

#### **Comparación de los tres**


Veamos lo mal que lo hace el de las variables no significativas y lo mucho que se parecen los otros dos, llegando a mejorar el de las variables significativas al total en alguno de los aspectos.

```{r}
n_trees_rf_bucle_imp = c(500,1000,2000) #Numero de árboles a utilizar
mtry_rf_bucle_imp = c(round(0.5*sqrt(ncol(datos)-2)), round(sqrt(ncol(datos)-2)))
min_nod_size_rf_imp = 1 #Queremos un fullgrown tree, para reducir el sesgo

tabla_comparacion_imp = data.frame(Splits_por_nodo = rep(NA, length(n_trees_rf_bucle_imp)*length(mtry_rf_bucle_imp)), Numero_arboles = rep(NA,length(n_trees_rf_bucle_imp)*length(mtry_rf_bucle_imp)),
                               Porcentaje_de_aciertos = rep(NA,length(n_trees_rf_bucle_imp)*length(mtry_rf_bucle_imp)))
rf_tabla_imp = matrix(rep(NA,length(n_trees_rf_bucle_imp)*length(mtry_rf_bucle_imp)), nrow = length(n_trees_rf_bucle_imp))

tabla_comparacion_noimp = data.frame(Splits_por_nodo = rep(NA, length(n_trees_rf_bucle_imp)*length(mtry_rf_bucle_imp)), Numero_arboles = rep(NA,length(n_trees_rf_bucle_imp)*length(mtry_rf_bucle_imp)),
                                   Porcentaje_de_aciertos = rep(NA,length(n_trees_rf_bucle_imp)*length(mtry_rf_bucle_imp)))
rf_tabla_noimp = matrix(rep(NA,length(n_trees_rf_bucle_imp)*length(mtry_rf_bucle_imp)), nrow = length(n_trees_rf_bucle_imp))

k = 0

for (i in 1:length(n_trees_rf_bucle_imp)) {
  for (j in 1:length(mtry_rf_bucle_imp)) {
    n_trees_int_imp = n_trees_rf_bucle_imp[i]
    mtry_int_imp = mtry_rf_bucle_imp[j]
    r_forest_buc_imp = ranger(
      formula = Revenue ~ Administrative + Administrative_Duration + Informational
      + Informational_Duration +  ProductRelated + ProductRelated_Duration
      + BounceRates + ExitRates + PageValues +  Month + VisitorType,
      data           = datos, 
      num.trees      = n_trees_int_imp,
      importance     = "impurity_corrected",
      mtry           = mtry_int_imp ,
      seed           = 8594,
      classification = TRUE)
    r_forest_buc_noimp = ranger(
      formula = Revenue ~ SpecialDay + OperatingSystems + Browser + Region +  TrafficType
      + Weekend,
      data           = datos, 
      num.trees      = n_trees_int_imp,
      importance     = "impurity_corrected",
      mtry           = mtry_int_imp ,
      seed           = 8594,
      classification = TRUE)
    rf_tabla_imp[i,j] = round((1-r_forest_buc_imp$prediction.error)*100, digits=3)
    k = k + 1
    tabla_comparacion_imp$Porcentaje_de_aciertos[k] = round((1-r_forest_buc_imp$prediction.error)*100, digits= 3)
    tabla_comparacion_imp$Splits_por_nodo[k] = mtry_rf_bucle_imp[j]
    tabla_comparacion_imp$Numero_arboles[k]  = n_trees_rf_bucle_imp[i]
    rf_tabla_noimp[i,j] = round((1-r_forest_buc_noimp$prediction.error)*100, digits=3)
    tabla_comparacion_noimp$Porcentaje_de_aciertos[k] = round((1-r_forest_buc_noimp$prediction.error)*100, digits= 3)
    tabla_comparacion_noimp$Splits_por_nodo[k] = mtry_rf_bucle_imp[j]
    tabla_comparacion_noimp$Numero_arboles[k]  = n_trees_rf_bucle_imp[i]
    
  }
}    
``` 

Aclaremos todo esto con alguna gráfica:

```{r}
library(ggplot2)
v2_rf = rf_tabla[,1][c(3,5,6)]
v4_rf = rf_tabla[,2][c(3,5,6)]

v2_rf_imp = rf_tabla_imp[,1]
v4_rf_imp = rf_tabla_imp[,2]

v2_rf_noimp = rf_tabla_noimp[,1]
v4_rf_noimp = rf_tabla_noimp[,2]

datos_grafica_imp =  data.frame( "Sub_imp" = v2_rf_imp,
                             "Acc_imp" = v4_rf_imp,
                             "Sub_noimp" = v2_rf_noimp,
                             "Acc_noimp" = v4_rf_noimp,
                             "Numero_de_arboles" = n_trees_rf_bucle_imp,
                             "Sub" = v2_rf,
                             "Acc" = v4_rf)
datos_grafica_imp
ggplot(datos_grafica_imp) +
  geom_line(aes(x = Numero_de_arboles, y = Sub_imp, col = "2 var por split (Importantes)")) + 
  geom_line(aes(x = Numero_de_arboles, y = Acc_imp, col = "4 var por split (Importantes)")) +
  geom_line(aes(x = Numero_de_arboles, y = Sub_noimp, col = "2 var por split (No Importantes)")) +
  geom_line(aes(x = Numero_de_arboles, y = Acc_noimp, col = "4 var por split(No Importantes)")) + 
  geom_line(aes(x = Numero_de_arboles, y = Sub, col = "2 var por split (TOTALES)")) + 
  geom_line(aes(x = Numero_de_arboles, y = Acc, col = "4 var por split (TOTALES)")) +
  scale_color_discrete("Número de variables por Split") +
  labs(title='Comparación entre los tres diferentes bosques',
       x='Número de árboles',
       y='Probabilidad de acierto') 
``` 

#### **Comparación entre el total y el modelo reducido**

Generamos primero las observaciones que nos faltan:

```{r}
n_trees_rf_bucle_imp2 = c(500,1000,2000) #Numero de árboles a utilizar
mtry_rf_bucle_imp2 = c(round(0.5*sqrt(ncol(datos)-2)), round(sqrt(ncol(datos)-2)),8,6)
min_nod_size_rf_imp2 = 1 #Queremos un fullgrown tree, para reducir el sesgo

tabla_comparacion_imp2 = data.frame(Splits_por_nodo = rep(NA, length(n_trees_rf_bucle_imp2)*length(mtry_rf_bucle_imp2)), Numero_arboles = rep(NA,length(n_trees_rf_bucle_imp2)*length(mtry_rf_bucle_imp2)),
                                   Porcentaje_de_aciertos = rep(NA,length(n_trees_rf_bucle_imp2)*length(mtry_rf_bucle_imp2)))
rf_tabla_imp2 = matrix(rep(NA,length(n_trees_rf_bucle_imp2)*length(mtry_rf_bucle_imp2)), nrow = length(n_trees_rf_bucle_imp2))

k = 0

for (i in 1:length(n_trees_rf_bucle_imp2)) {
  for (j in 1:length(mtry_rf_bucle_imp2)) {
    n_trees_int_imp2 = n_trees_rf_bucle_imp2[i]
    mtry_int_imp2 = mtry_rf_bucle_imp2[j]
    r_forest_buc_imp2 = ranger(
      formula = Revenue ~ Administrative + Administrative_Duration + Informational
      + Informational_Duration +  ProductRelated + ProductRelated_Duration
      + BounceRates + ExitRates + PageValues +  Month + VisitorType,
      data           = datos, 
      num.trees      = n_trees_int_imp2,
      importance     = "impurity_corrected",
      mtry           = mtry_int_imp2 ,
      seed           = 8594,
      classification = TRUE)
    
    rf_tabla_imp2[i,j] = round((1-r_forest_buc_imp2$prediction.error)*100, digits=3)
    k = k + 1
    tabla_comparacion_imp2$Porcentaje_de_aciertos[k] = round((1-r_forest_buc_imp2$prediction.error)*100, digits= 3)
    tabla_comparacion_imp2$Splits_por_nodo[k] = mtry_rf_bucle_imp2[j]
    tabla_comparacion_imp2$Numero_arboles[k]  = n_trees_rf_bucle_imp2[i]
    
  }
}    
```

Veamos ahora la comparación gráfica:
```{r}
v2_rf = rf_tabla[,1][c(3,5,6)]
v4_rf = rf_tabla[,2][c(3,5,6)]
v8_rf = rf_tabla[,3][c(3,5,6)]
v6_rf = rf_tabla[,4][c(3,5,6)]

v2_rf_imp2 = rf_tabla_imp2[,1]
v4_rf_imp2 = rf_tabla_imp2[,2]
v8_rf_imp2 = rf_tabla_imp2[,3]
v6_rf_imp2 = rf_tabla_imp2[,4]

datos_grafica_imp2 =  data.frame( "Sub" = v2_rf,
                             "Acc" = v4_rf,
                             "Sup" = v8_rf,
                             "Una" = v6_rf,
                             "Sub_imp2" = v2_rf_imp2,
                             "Acc_imp2" = v4_rf_imp2,
                             "Sup_imp2" = v8_rf_imp2,
                             "Una_imp2" = v6_rf_imp2,
                             "Numero_de_arboles" = n_trees_rf_bucle_imp2)
ggplot(datos_grafica_imp2) +
  geom_line(aes(x = Numero_de_arboles, y = Sub, col = "2 var por split")) + 
  geom_line(aes(x = Numero_de_arboles, y = Acc, col = "4 var por split")) +
  geom_line(aes(x = Numero_de_arboles, y = Una, col = "6 var por split")) +
  geom_line(aes(x = Numero_de_arboles, y = Sup, col = "8 var por split")) + 
  geom_line(aes(x = Numero_de_arboles, y = Sub_imp2, col = "2 var por split (Modelo reducido)")) + 
  geom_line(aes(x = Numero_de_arboles, y = Acc_imp2, col = "4 var por split (Modelo reducido)")) +
  geom_line(aes(x = Numero_de_arboles, y = Una_imp2, col = "6 var por split (Modelo reducido)")) +
  geom_line(aes(x = Numero_de_arboles, y = Sup_imp2, col = "8 var por split (Modelo reducido)")) + 
  scale_color_discrete("Número de variables por Split") +
  labs(title='Comparación entre modelo total y reducido',
       x='Número de árboles',
       y='Probabilidad de acierto')
```

Como podemos observar, la diferencia entre el modelo reducido y el modelo general es casi inapreciable y se mueve en una misma franja, en la que ninguno es mejor que el otro, ya que depende del numero de splits y de árboles. La diferencia más grande se observa entre los splits que elijen solo dos variables.